{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from torch.autograd import Variable\n",
        "from sklearn.metrics import f1_score\n",
        "import os\n",
        "import torch\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "dd2QT1Jp0hpC"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Предсказание модели для тестовой выборки"
      ],
      "metadata": {
        "id": "ME4S44hrOuL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_text(x):\n",
        "    x = x.lower()\n",
        "    x = replace_contractions(x)\n",
        "    pattern = r'[^a-zA-z0-9\\s]'\n",
        "    x = re.sub(pattern, '', x)\n",
        "    if bool(re.search(r'\\d', x)):\n",
        "        x = re.sub('[0-9]{5,}', '#####', x)\n",
        "        x = re.sub('[0-9]{4}', '####', x)\n",
        "        x = re.sub('[0-9]{3}', '###', x)\n",
        "        x = re.sub('[0-9]{2}', '##', x)\n",
        "    return x\n",
        "\n",
        "contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
        "def _get_contractions(contraction_dict):\n",
        "    contraction_re = re.compile('(%s)' % '|'.join(contraction_dict.keys()))\n",
        "    return contraction_dict, contraction_re\n",
        "contractions, contractions_re = _get_contractions(contraction_dict)\n",
        "def replace_contractions(text):\n",
        "    def replace(match):\n",
        "        return contractions[match.group(0)]\n",
        "    return contractions_re.sub(replace, text)"
      ],
      "metadata": {
        "id": "OIZXrqXYzzYl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_solution = pd.read_csv(\"train_solution.csv\")\n",
        "train_data = pd.read_csv(\"train_data.csv\")\n",
        "train = train_data.merge(train_solution, on='id')\n",
        "train['tokenized'] = train['message'].apply(lambda x: clean_text(x))\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_X, test_X, train_y, test_y = train_test_split(train['tokenized'], train['category'],\n",
        "                                                    stratify=train['category'], \n",
        "                                                    test_size=0.25, random_state=42)\n",
        "from keras_preprocessing.text import Tokenizer\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "tokenizer = Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(list(train_X))\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "train_y = le.fit_transform(train_y.values)\n",
        "test_y = le.transform(test_y.values)"
      ],
      "metadata": {
        "id": "16T1StFa1Ls2"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_features = 16958\n",
        "embed_size = 300\n",
        "maxlen = 3160\n",
        "\n",
        "class CNN_Text(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN_Text, self).__init__()\n",
        "        filter_sizes = [1,2,3,5]\n",
        "        num_filters = 36\n",
        "        n_classes = 3\n",
        "        self.embedding = nn.Embedding(max_features, embed_size)\n",
        "        self.convs1 = nn.ModuleList([nn.Conv2d(1, num_filters, (K, embed_size)) for K in filter_sizes])\n",
        "        self.dropout = nn.Dropout(0.15)\n",
        "        self.fc1 = nn.Linear(len(filter_sizes)*num_filters, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x) \n",
        "        x = x.unsqueeze(1)  \n",
        "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1] \n",
        "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  \n",
        "        x = torch.cat(x, 1)\n",
        "        x = self.dropout(x)  \n",
        "        logit = self.fc1(x) \n",
        "        return logit\n",
        "    \n",
        "    #эмбеддинг текста\n",
        "    def get_embedding(self,x):\n",
        "        x = self.embedding(x) \n",
        "        x = x.unsqueeze(1)  \n",
        "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1] \n",
        "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  \n",
        "        x = torch.cat(x, 1)\n",
        "        return x"
      ],
      "metadata": {
        "id": "S4jFjL86zvBc"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#тестирование модели на CPU\n",
        "model = CNN_Text()\n",
        "model.load_state_dict(torch.load(\"/content/model_new1.pt\", map_location=torch.device('cpu')))  # Choose whatever GPU device number you want\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXdRAuqn0RZr",
        "outputId": "eac61f49-ae2b-43f5-c274-047d41cce728"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN_Text(\n",
              "  (embedding): Embedding(16958, 300)\n",
              "  (convs1): ModuleList(\n",
              "    (0): Conv2d(1, 36, kernel_size=(1, 300), stride=(1, 1))\n",
              "    (1): Conv2d(1, 36, kernel_size=(2, 300), stride=(1, 1))\n",
              "    (2): Conv2d(1, 36, kernel_size=(3, 300), stride=(1, 1))\n",
              "    (3): Conv2d(1, 36, kernel_size=(5, 300), stride=(1, 1))\n",
              "  )\n",
              "  (dropout): Dropout(p=0.15, inplace=False)\n",
              "  (fc1): Linear(in_features=144, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(x):    \n",
        "    x = clean_text(x)\n",
        "    x = tokenizer.texts_to_sequences([x])\n",
        "    x = pad_sequences(x, maxlen=maxlen)\n",
        "    x = torch.tensor(x, dtype=torch.long)\n",
        "    pred = model(x).detach()\n",
        "    pred = F.softmax(pred).cpu().numpy()\n",
        "    pred = pred.argmax(axis=1)\n",
        "    pred = le.classes_[pred]\n",
        "    return pred[0]"
      ],
      "metadata": {
        "id": "BDXKk9Yk00vA"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data_new = pd.read_csv(\"test_data.csv\")"
      ],
      "metadata": {
        "id": "vwE1j5SFM0Hv"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data_new['category'] = test_data_new['message'].apply(lambda x: predict(x))"
      ],
      "metadata": {
        "id": "DP5Zap11LJ7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data_new = test_data_new.drop(columns = [\"message\"])\n",
        "compression_opts = dict(method='zip',\n",
        "                        archive_name='test_data.csv')  \n",
        "test_data_new.to_csv('test_data2.zip', index=False,\n",
        "          compression=compression_opts)  "
      ],
      "metadata": {
        "id": "LPd3mhitKDXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Получение эмбеддингов текстов из обучающей выборки и эмбеддинга для фразы \"My future\""
      ],
      "metadata": {
        "id": "rhKIc3ryNgOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Эмбеддинг текста\n",
        "def get_emb(x, is_cleaned=False):\n",
        "  if not is_cleaned:\n",
        "    x = clean_text(x)\n",
        "  x = tokenizer.texts_to_sequences([x])\n",
        "  x = pad_sequences(x, maxlen=maxlen)\n",
        "  x = torch.tensor(x, dtype=torch.long)\n",
        "  return model.get_embedding(x)"
      ],
      "metadata": {
        "id": "vHKz8IR8e3ak"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_future_emb = get_emb(\"My future\")"
      ],
      "metadata": {
        "id": "0zV2FHd7j0QH"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Признаковое предствление для фразы \"My future\""
      ],
      "metadata": {
        "id": "jcZ6wKSIO2Qf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_future_emb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUhkJMNtKQ93",
        "outputId": "54c9ba29-e49f-4697-ed67-7184bcf2f84d"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.5005, 0.7324, 0.0200, 0.0000, 0.4328, 0.0874, 0.6357, 0.0000, 0.4468,\n",
              "         0.0000, 0.5672, 0.6594, 2.5821, 0.3509, 0.5934, 0.0000, 0.6595, 3.3656,\n",
              "         0.0000, 1.1884, 0.1779, 0.5806, 1.1113, 0.0000, 0.8545, 0.4795, 0.0000,\n",
              "         3.9702, 0.2404, 0.6566, 0.0745, 0.6073, 0.3120, 0.1291, 0.6322, 0.0000,\n",
              "         3.3630, 1.5500, 0.5827, 0.1330, 0.0844, 0.7993, 0.3649, 2.4431, 0.0544,\n",
              "         0.0000, 1.3672, 0.0000, 0.8623, 0.0000, 0.2996, 0.0000, 0.1910, 2.3301,\n",
              "         2.8495, 1.8663, 0.1881, 0.3238, 0.0535, 0.0000, 1.3020, 0.5230, 0.0000,\n",
              "         0.8869, 0.5742, 0.8364, 1.8859, 2.1713, 0.0692, 2.4143, 2.0217, 0.0310,\n",
              "         2.1670, 1.3856, 1.0410, 2.3380, 0.2244, 0.0000, 0.0000, 0.0000, 3.3635,\n",
              "         0.0000, 2.6294, 0.0000, 0.0000, 1.9361, 0.3242, 0.0000, 1.6005, 0.9955,\n",
              "         0.0000, 0.0000, 1.8530, 0.3508, 1.8784, 0.8346, 0.0000, 0.0186, 3.8435,\n",
              "         0.0000, 0.6887, 0.9910, 0.0000, 0.0000, 0.3850, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 2.0138, 0.0000, 0.0000, 0.3009, 0.0000, 1.3650, 0.0000, 0.0000,\n",
              "         0.0000, 0.8994, 0.0000, 0.0000, 1.9956, 0.9320, 1.9832, 0.5268, 4.3731,\n",
              "         0.0000, 2.9814, 2.9530, 0.0000, 1.5109, 0.0000, 0.3318, 0.0000, 0.9579,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Получаем эмбеддинги текстов из обучающей выборки\n",
        "emb_of_texts = {}\n",
        "for index, row in train.iterrows():\n",
        "  emb_of_texts[row['id']] = get_emb(row['tokenized'], is_cleaned=True)"
      ],
      "metadata": {
        "id": "aoXtEPh4IJAc"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Ближайший текст к исходному тексту - тот, чье косинусное сходство с исходным текстом максимально\n",
        "#аналог most similar в word2vec\n",
        "def get_most_similar(word):\n",
        "  max_similarity = 0\n",
        "  nearest_text = \"\"\n",
        "  word_emb = get_emb(word)\n",
        "  for index, row in train.iterrows():\n",
        "    emb_of_text = emb_of_texts[row['id']]\n",
        "    similarity = cosine_similarity(word_emb, emb_of_text)[0][0]\n",
        "    if similarity > max_similarity:\n",
        "      nearest_text = row['message']\n",
        "      max_similarity = similarity\n",
        "  \n",
        "  return nearest_text, max_similarity"
      ],
      "metadata": {
        "id": "DTWDAs1ljSPE"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nearest_text, max_similarity = get_most_similar(\"My future\")"
      ],
      "metadata": {
        "id": "MgKFzT0BAMYH"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nearest_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "AxX8A3ayJneX",
        "outputId": "e8dd12bf-9043-4b60-c0be-67ff9fa7fbec"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'my bad'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_similarity"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcUa93xqJqX_",
        "outputId": "bcdd810c-142f-4b40-d939-42317a4a841a"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9678832"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Надеюсь, модель врет))"
      ],
      "metadata": {
        "id": "oiDklDQONrIN"
      }
    }
  ]
}